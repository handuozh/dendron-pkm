---
id: 93f47e4f-849b-45cc-b250-e3d674c20b7f
title: Keypoint-detection-and-description
desc: ''
updated: 1606203942337
created: 1606196809674
parent: 2b4ada79-0c24-4eb3-85a9-71aee1c0ddbe
children: []
fname: pkm.Keypoint.keypoint-detection-and-description
hpath: pkm.Keypoint.keypoint-detection-and-description
---
# Keypoint Detection and Description Simultaneously

- tags: [detection-and-describe](1a60b043-5b79-4df5-a1f2-d92db7b4e8d3)

还是和之前一样先说。不出意外，现在几乎都是detector和descriptor一起去学了，这个任务进展非常大。个人觉得比较有意思的几个工作：首先是CVPR'19的[pkm.D2-Net](5bc96717-b624-46ed-afff-6c08666793a2)(<https://github.com/mihaidusmanu/d2-net)，这篇大意是说detector其实没必要特意去学，直接从descriptor里取depth-wise> maximum和local spatial maximum就可以得到。如果沿着这个方向做，甚至detector都没必要特意设计loss去学（这个loss真的挺难设计的），确实是个很吸引人的性质。但现在d2-net的keypoint localization还是很不准，这对geometry computation影响还是很大，所以如果用来做sfm或者slam这种对geometry很敏感的任务估计还不太行。最近还有一篇[pkm.R2D2](a57da98a-ba3b-4ff0-bd63-df3de73deb55) (<https://arxiv.org/abs/1906.06195>) 会比d2-net复杂一些，不过看实验结果似乎localization error小了很多，如果以后开源可以多测下。不过D2-Net和R2D2都依赖dense gt correspondences, 比如D2-Net依赖MegaDepth，而R2D2是用EpicFlow自己插值出来的，获取成本和精度都是麻烦事...所以最近有一篇[pkm.Unsuperpoint](e33d889e-271d-4140-a9ac-ebe5c511c502)，完全[pkm.Self-supervised](f7836a8a-f790-4c84-b3b4-5f12f2f75160)方法只用homography transformation（这点其实和[pkm.Superpoint](52dc650e-80ec-49d3-a804-23df714f1469)很像，不过做成了self-improving，比[pkm.Superpoint](52dc650e-80ec-49d3-a804-23df714f1469)优雅）去做，看起来效果也相当不错，期待它开源了再仔细测一测了。

